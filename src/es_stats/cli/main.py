from __future__ import annotationsimport argparseimport loggingimport sysfrom pathlib import Pathfrom zoneinfo import ZoneInfofrom es_stats.config.settings import settingsfrom es_stats.db.connection import connect_defaultfrom es_stats.logging import configure_loggingfrom es_stats.repositories.sql_loader import load_sqlfrom es_stats.services.csv_parser import CsvValidationError, read_bars_csvfrom es_stats.services.time_fields import compute_time_fieldslogger = logging.getLogger(__name__)def init_db() -> int:    sql = load_sql("schema/001_init.sql")    with connect_default() as conn:        conn.executescript(sql)    logger.info("Initialized database at %s", settings.db_path)    return 0def _validate_timezone(tz_name: str, parser: argparse.ArgumentParser) -> None:    try:        ZoneInfo(tz_name)    except Exception:        parser.error(f"Invalid timezone: {                     tz_name!r}. Must be an IANA name like 'America/Chicago'.")def _validate_import_args(args: argparse.Namespace, parser: argparse.ArgumentParser) -> None:    csv_path = Path(args.file)    if not csv_path.exists() or not csv_path.is_file():        parser.error(f"CSV file not found: {str(csv_path)!r}")    if csv_path.suffix.lower() != ".csv":        parser.error(f"Expected a .csv file, got: {csv_path.name!r}")    _validate_timezone(args.timezone, parser)    # argparse choices already enforces merge_policy; this is just defensive.    if args.merge_policy not in ("skip", "overwrite"):        parser.error("merge-policy must be one of: skip, overwrite")def import_csv_contract_only(args: argparse.Namespace, parser: argparse.ArgumentParser) -> int:    """    Phase 3.2â€“3.3: validate import arguments, parse/validate CSV (in-memory),    and compute time-derived fields (still no DB writes).    Includes a non-blocking sanity check on the typical bar interval:    - derive ts_start_utc for each row    - compute deltas between consecutive timestamps    - log a WARNING if the median delta is not 60 seconds (canonical 1-minute bars)    This check is advisory in Phase 3.3; it does not fail the command.    """    _validate_import_args(args, parser)    try:        bars = read_bars_csv(Path(args.file))    except CsvValidationError as e:        parser.error(str(e))    # Phase 3.3: compute derived time fields per row    derived = [compute_time_fields(b.dt, args.timezone) for b in bars]    # Advisory interval check (median delta of consecutive timestamps)    median_delta = None    if len(derived) >= 2:        ts_sorted = sorted(t.ts_start_utc for t in derived)        deltas = [b - a for a, b in zip(ts_sorted, ts_sorted[1:]) if b > a]        if deltas:            deltas.sort()            median_delta = deltas[len(deltas) // 2]            if median_delta != 60:                logger.warning(                    "Detected median bar interval of %ss; canonical expected 60s. "                    "Input file may be %sm bars or contain significant gaps.",                    median_delta,                    median_delta // 60 if median_delta % 60 == 0 else -1,                )    ts_min = min(t.ts_start_utc for t in derived) if derived else None    ts_max = max(t.ts_start_utc for t in derived) if derived else None    td_min = min(t.trading_date_ct_int for t in derived) if derived else None    td_max = max(t.trading_date_ct_int for t in derived) if derived else None    logger.info(        "Import parse OK: file=%s symbol=%s timezone=%s merge_policy=%s rows=%d "        "ts_min=%s ts_max=%s trading_date_ct=%s..%s median_delta_s=%s",        args.file,        args.symbol,        args.timezone,        args.merge_policy,        len(bars),        ts_min,        ts_max,        td_min,        td_max,        median_delta,    )    logger.info(        "Phase 3.3: parsed/validated + time-derived only (no DB writes yet).")    return 0def build_parser() -> argparse.ArgumentParser:    parser = argparse.ArgumentParser(prog="es-stats")    sub = parser.add_subparsers(dest="command", required=True)    # init-db    p_init = sub.add_parser(        "init-db", help="Create schema in the configured SQLite database.")    p_init.set_defaults(_handler="init-db")    # import-csv (contract only)    p_import = sub.add_parser(        "import-csv", help="Validate import arguments (Phase 3.1).")    p_import.add_argument(        "-f", "--file",        required=True,        help="Path to CSV file (server/admin input).",    )    p_import.add_argument(        "-s", "--symbol",        required=True,        help="Instrument symbol (e.g., ES, NQ).",    )    p_import.add_argument(        "-t", "--timezone",        default="America/Chicago",        help="Timezone used to interpret timestamps in the CSV (default: America/Chicago).",    )    p_import.add_argument(        "-m", "--merge-policy",        default="skip",        choices=["skip", "overwrite"],        help="On duplicate bar keys, either skip or overwrite existing records.",    )    p_import.set_defaults(_handler="import-csv")    return parserdef main(argv: list[str] | None = None) -> int:    configure_logging(settings.log_level)    parser = build_parser()    args = parser.parse_args(sys.argv[1:] if argv is None else argv)    handler = getattr(args, "_handler", None)    if handler == "init-db":        return init_db()    if handler == "import-csv":        return import_csv_contract_only(args, parser)    # Should never happen with required subparser + set_defaults, but keep it explicit.    logger.error("No handler found for command: %s", args.command)    return 2if __name__ == "__main__":    raise SystemExit(main())