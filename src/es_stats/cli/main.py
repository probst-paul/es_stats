from __future__ import annotationsimport argparseimport loggingimport sysimport timefrom pathlib import Pathfrom zoneinfo import ZoneInfofrom es_stats.config.settings import settingsfrom es_stats.db.connection import connect_defaultfrom es_stats.logging import configure_loggingfrom es_stats.repositories.sql_loader import load_sqlfrom es_stats.services.csv_parser import CsvValidationError, read_bars_csvfrom es_stats.services.time_fields import compute_time_fieldsfrom es_stats.repositories.imports_repo import finalize_import_run, insert_import_runfrom es_stats.repositories.instruments_repo import ensure_instrumentlogger = logging.getLogger(__name__)def init_db() -> int:    sql = load_sql("schema/001_init.sql")    with connect_default() as conn:        conn.executescript(sql)    logger.info("Initialized database at %s", settings.db_path)    return 0def _validate_timezone(tz_name: str, parser: argparse.ArgumentParser) -> None:    try:        ZoneInfo(tz_name)    except Exception:        parser.error(f"Invalid timezone: {                     tz_name!r}. Must be an IANA name like 'America/Chicago'.")def _validate_import_args(args: argparse.Namespace, parser: argparse.ArgumentParser) -> None:    csv_path = Path(args.file)    if not csv_path.exists() or not csv_path.is_file():        parser.error(f"CSV file not found: {str(csv_path)!r}")    if csv_path.suffix.lower() != ".csv":        parser.error(f"Expected a .csv file, got: {csv_path.name!r}")    _validate_timezone(args.timezone, parser)    # argparse choices already enforces merge_policy; this is just defensive.    if args.merge_policy not in ("skip", "overwrite"):        parser.error("merge-policy must be one of: skip, overwrite")def import_csv_contract_only(args: argparse.Namespace, parser: argparse.ArgumentParser) -> int:    """    Phase 3.4: validate args, parse/validate CSV, compute time-derived fields,    and persist an import audit row (imports). Still no bar writes.    Includes a non-blocking sanity check on typical bar interval:    compute median delta of consecutive ts_start_utc and warn if != 60.    """    _validate_import_args(args, parser)    started_at_utc = int(time.time())    source_name = Path(args.file).name    # We will record detected interval in the import audit.    median_delta = None    with connect_default() as conn:        # Ensure instrument exists        instrument_id = ensure_instrument(conn, args.symbol)        # Insert audit row early so failures are captured        import_id = insert_import_run(conn, {            "instrument_id": instrument_id,            "source_name": source_name,            "source_hash": None,  # can add later            "input_timezone": args.timezone,            "bar_interval_seconds": 60,  # updated after we compute median delta            "merge_policy": args.merge_policy,            "started_at_utc": started_at_utc,            # We start as failed and flip to success only on full completion.            # This avoids leaving a "success" row if the process crashes mid-run.            "status": "failed",            "error_summary": None,        })        try:            # Parse/validate CSV (in-memory only)            try:                bars = read_bars_csv(Path(args.file))            except CsvValidationError as e:                parser.error(str(e))  # raises SystemExit(2)            # Compute derived time fields (Phase 3.3)            derived = [compute_time_fields(b.dt, args.timezone) for b in bars]            # Advisory interval check (median delta)            if len(derived) >= 2:                ts_sorted = sorted(t.ts_start_utc for t in derived)                deltas = [b - a for a,                          b in zip(ts_sorted, ts_sorted[1:]) if b > a]                if deltas:                    deltas.sort()                    median_delta = deltas[len(deltas) // 2]                    if median_delta != 60:                        logger.warning(                            "Detected median bar interval of %ss; canonical expected 60s. "                            "Input file may be %sm bars or contain significant gaps.",                            median_delta,                            median_delta // 60 if median_delta % 60 == 0 else -1,                        )            ts_min = min(t.ts_start_utc for t in derived) if derived else None            ts_max = max(t.ts_start_utc for t in derived) if derived else None            td_min = min(                t.trading_date_ct_int for t in derived) if derived else None            td_max = max(                t.trading_date_ct_int for t in derived) if derived else None            finished_at_utc = int(time.time())            # Finalize audit row (no bars written yet, so inserted/updated/rejected are 0)            finalize_import_run(conn, {                "import_id": import_id,                "finished_at_utc": finished_at_utc,                "ts_min_utc": ts_min,                "ts_max_utc": ts_max,                "row_count_read": len(bars),                "row_count_inserted": 0,                "row_count_updated": 0,                "row_count_rejected": 0,                "status": "success",                "error_summary": None,            })            # Update interval seconds on the import row (reuse finalize to set bar_interval_seconds later would require schema change)            # Since bar_interval_seconds is part of imports, simplest is: update it directly here.            if median_delta is not None:                conn.execute(                    "UPDATE imports SET bar_interval_seconds = ? WHERE import_id = ?;",                    (int(median_delta), int(import_id)),                )            logger.info(                "Import audit OK: import_id=%s file=%s symbol=%s rows=%d ts_min=%s ts_max=%s trading_date_ct=%s..%s median_delta_s=%s",                import_id, args.file, args.symbol, len(                    bars), ts_min, ts_max, td_min, td_max, median_delta            )            logger.info("Phase 3.4: audit persisted (no bar writes yet).")            return 0        except SystemExit:            # parser.error already raised; make sure import row exists and is marked failed            finalize_import_run(conn, {                "import_id": import_id,                "finished_at_utc": int(time.time()),                "ts_min_utc": None,                "ts_max_utc": None,                "row_count_read": 0,                "row_count_inserted": 0,                "row_count_updated": 0,                "row_count_rejected": 0,                "status": "failed",                "error_summary": "Argument/CSV validation failed.",            })            raise        except Exception as e:            finalize_import_run(conn, {                "import_id": import_id,                "finished_at_utc": int(time.time()),                "ts_min_utc": None,                "ts_max_utc": None,                "row_count_read": 0,                "row_count_inserted": 0,                "row_count_updated": 0,                "row_count_rejected": 0,                "status": "failed",                "error_summary": str(e)[:500],            })            raisedef build_parser() -> argparse.ArgumentParser:    parser = argparse.ArgumentParser(prog="es-stats")    sub = parser.add_subparsers(dest="command", required=True)    # init-db    p_init = sub.add_parser(        "init-db", help="Create schema in the configured SQLite database.")    p_init.set_defaults(_handler="init-db")    # import-csv (contract only)    p_import = sub.add_parser(        "import-csv", help="Validate import arguments (Phase 3.1).")    p_import.add_argument(        "-f", "--file",        required=True,        help="Path to CSV file (server/admin input).",    )    p_import.add_argument(        "-s", "--symbol",        required=True,        help="Instrument symbol (e.g., ES, NQ).",    )    p_import.add_argument(        "-t", "--timezone",        default="America/Chicago",        help="Timezone used to interpret timestamps in the CSV (default: America/Chicago).",    )    p_import.add_argument(        "-m", "--merge-policy",        default="skip",        choices=["skip", "overwrite"],        help="On duplicate bar keys, either skip or overwrite existing records.",    )    p_import.set_defaults(_handler="import-csv")    return parserdef main(argv: list[str] | None = None) -> int:    configure_logging(settings.log_level)    parser = build_parser()    args = parser.parse_args(sys.argv[1:] if argv is None else argv)    handler = getattr(args, "_handler", None)    if handler == "init-db":        return init_db()    if handler == "import-csv":        return import_csv_contract_only(args, parser)    # Should never happen with required subparser + set_defaults, but keep it explicit.    logger.error("No handler found for command: %s", args.command)    return 2if __name__ == "__main__":    raise SystemExit(main())