from __future__ import annotationsimport sqlite3from pathlib import Pathfrom es_stats.repositories.sql_loader import load_sqlfrom es_stats.repositories.instruments_repo import ensure_instrumentfrom es_stats.repositories.imports_repo import insert_import_runfrom es_stats.repositories.bars_30m_repo import rebuild_bars_30m_rangedef _init_db(db_path: Path) -> sqlite3.Connection:    conn = sqlite3.connect(str(db_path))    conn.execute("PRAGMA foreign_keys = ON;")    conn.executescript(load_sql("schema/001_init.sql"))    return conndef test_rebuild_bars_30m_single_bucket(tmp_path: Path):    conn = _init_db(tmp_path / "t.sqlite3")    try:        instrument_id = ensure_instrument(conn, "ES")        import_id = insert_import_run(conn, {            "instrument_id": instrument_id,            "source_name": "test.csv",            "source_hash": None,            "input_timezone": "America/Chicago",            "bar_interval_seconds": 60,            "merge_policy": "skip",            "started_at_utc": 1700000000,            "status": "failed",            "error_summary": None,        })        # Build 30 one-minute bars in the same 30-minute bucket.        # trading_date_ct_int is arbitrary for test; ct_minute_of_day bucket = 510 (08:30)        td = 20250101        bucket_min = 510        # ts_start_utc values just need to be consecutive and unique in this test        ts0 = 1700000000        rows = []        for i in range(30):            rows.append((                instrument_id,                ts0 + i * 60,                td,                bucket_min + i,  # 510..539                100.0 + i,       # open                101.0 + i,       # high                99.0 + i,        # low                100.5 + i,       # close                1,               # volume                import_id,            ))        conn.executemany(            """            INSERT INTO bars_1m (              instrument_id, ts_start_utc, trading_date_ct_int, ct_minute_of_day,              open, high, low, close, volume, source_import_id            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);            """,            rows,        )        counts = rebuild_bars_30m_range(            conn,            instrument_id=instrument_id,            td_min=td,            td_max=td,            derived_from_import_id=import_id,        )        assert counts.inserted == 1        r = conn.execute(            """            SELECT bucket_ct_minute_of_day, open, high, low, close, volume, bar_count_1m, is_complete, session, period_index            FROM bars_30m            WHERE instrument_id = ? AND trading_date_ct_int = ?;            """,            (instrument_id, td),        ).fetchone()        assert int(r[0]) == 510        assert float(r[1]) == 100.0          # open = first 1m open        assert float(r[2]) == 130.0          # high = max of highs (101..130)        assert float(r[3]) == 99.0           # low = min of lows        assert float(r[4]) == 129.5          # close = last 1m close        assert int(r[5]) == 30               # volume sum        assert int(r[6]) == 30               # bar_count_1m        assert int(r[7]) == 1                # is_complete        assert r[8] == "RTH"        assert int(r[9]) == 0                # first RTH bucket index    finally:        conn.close()def test_rebuild_bars_30m_partial_bucket(tmp_path: Path):    conn = _init_db(tmp_path / "t.sqlite3")    try:        instrument_id = ensure_instrument(conn, "ES")        import_id = insert_import_run(conn, {            "instrument_id": instrument_id,            "source_name": "test.csv",            "source_hash": None,            "input_timezone": "America/Chicago",            "bar_interval_seconds": 60,            "merge_policy": "skip",            "started_at_utc": 1700000000,            "status": "failed",            "error_summary": None,        })        # Partial bucket: 12 one-minute bars in the 08:30 bucket (510..521)        td = 20250101        bucket_min = 510        ts0 = 1700000000        rows = []        for i in range(12):            rows.append((                instrument_id,                ts0 + i * 60,                td,                bucket_min + i,  # 510..521                200.0 + i,       # open                201.0 + i,       # high                199.0 + i,       # low                200.5 + i,       # close                2,               # volume                import_id,            ))        conn.executemany(            """            INSERT INTO bars_1m (              instrument_id, ts_start_utc, trading_date_ct_int, ct_minute_of_day,              open, high, low, close, volume, source_import_id            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);            """,            rows,        )        counts = rebuild_bars_30m_range(            conn,            instrument_id=instrument_id,            td_min=td,            td_max=td,            derived_from_import_id=import_id,        )        assert counts.inserted == 1        r = conn.execute(            """            SELECT bucket_ct_minute_of_day, open, high, low, close, volume,                   bar_count_1m, is_complete, session, period_index            FROM bars_30m            WHERE instrument_id = ? AND trading_date_ct_int = ?;            """,            (instrument_id, td),        ).fetchone()        assert int(r[0]) == 510        assert float(r[1]) == 200.0          # open = first 1m open        assert float(r[2]) == 212.0          # high = max highs (201..212)        assert float(r[3]) == 199.0          # low = min lows        assert float(r[4]) == 211.5          # close = last close (200.5+11)        assert int(r[5]) == 24               # volume sum (12 * 2)        assert int(r[6]) == 12               # bar_count_1m        assert int(r[7]) == 0                # is_complete (not 30)        assert r[8] == "RTH"        assert int(r[9]) == 0    finally:        conn.close()